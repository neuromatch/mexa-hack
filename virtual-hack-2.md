# MEXA Virtual Hackathon 


## Mental Health and AI: Intervention

### January 26 - 29 
[Join MEXA here.](https://mexa.app/)  
**[Register Here!](https://airtable.com/app3qpCRLWwTWEy6S/pagOvhT4clRFrygZ9/form)**

Teams will have from 0:00 UTC on January 26 to 24:00 UTC on January 29 to work on the challenge. You can see what time that is in your time zone [here](https://www.worldtimebuddy.com/). Depending on your time zone, it might start on January 25! 

This hackathon is funded by Wellcome!

[You can find Rules and Judging Criteria here.](https://github.com/neuromatch/mexa-hack/blob/main/rules.md) 

### Timeline of Events
* Kick-Off Event: Friday, January 24 at 3 p.m. UTC
* Hacking: January 26 at 00:00 UTC - January 29 at 24:00 UTC
* Winners Celebration: Monday, February 3 at 5 p.m. UTC

We added an additional day to this Hackathon and moved one of the days to be a weekend day. Our hope is you'll have a little more time to meet your team, and have more flexibility to contribute based on your schedule.

### Background
Access to mental health care remains a global challenge. Many people, especially in under-resourced regions, do not have access to trained therapists or face social and cultural obstacles that prevent them from seeking help. The global shortage of mental health professionals exacerbates these problems, leaving many individuals without the support they need.

AI and large language models (LLMs) have shown potential in assisting with various tasks that require language understanding and generation, such as answering questions, offering advice, or simulating conversational agents. Given these capabilities, AI could offer new or improved pathways for providing mental health support and supporting mental health professionals.

However, LLMs are not therapists. While they can engage in conversations, they have not been designed to offer mental health care. For example, they lack the emotional intelligence, cultural sensitivity, and nuanced understanding of human psychology that trained professionals bring. To effectively use AI in therapeutic contexts, it’s crucial to explore how LLMs can complement or enhance mental health care without overstepping their capabilities, and to understand the shortcomings or unintended consequences of using them in treatment

This challenge invites participants to explore whether and how LLMs could safely make mental health care more efficient and effective. That could involve developing specialised models that are better capable of acting as therapeutic agents; exploring new methods of collaboration between AI, trained health professionals, and patients; or anything else that addresses an important gap in making AI-assisted mental health intervention safe, efficient, and effective. All projects should engage with Lived Experience expertise to ensure that solutions created are shaped by the priorities of those who will most benefit, and that interventions are safe, ethical and effective.


### Objective
Design a solution that uses generative AI to safely improve the efficiency and/or effectiveness of human-delivered mental health care. Your solution should explore the following key areas:
1. **Access and Availability.**
   
Explore how AI-driven systems could improve access to mental health care, particularly for individuals who face barriers such as geographic isolation, financial constraints, or cultural obstacles. What role could LLMs play in making therapeutic interventions more widely or easily accessible? For example, can they be used as a first line of support before professional intervention is available, and how could this reduce strain on mental health systems?

2. **Model Capabilities.**

Evaluate the current capabilities of LLMs in providing therapeutic support. Can LLMs, as they exist today, effectively provide mental health support? Why or why not? If not, how could they be improved such that they can offer mental health support? Examples of things to consider include:
* Memory: Therapists use past sessions to build rapport and trust over time, and they remember key facts and incidents to inform their approach. How could LLMs make similar use of memory to offer better support?
* Emotions: A key aspect of therapy is recognising and understanding emotions in others. How might an AI model recognize and appropriately respond to emotions expressed by users?
* Affirmation vs Challenge: Therapists often challenge their patients, but LLMs tend to be affirmative in nature. How can these models learn when and how to challenge a user in a supportive and safe way?
* Cultural Sensitivity: Mental health discussions vary significantly across cultures. Can an LLM adapt to these cultural nuances, or how could it be trained to do so?

3. **Human-AI Interfaces.**

Many applications of generative AI default to a familiar chatbot interface. But is this the most appropriate interface between LLMs, clinicians, and patients in mental health? What do clinicians and patients need from their interactions with your models, and how can you design a user interface that meets those needs? We encourage you to not default to a chatbot for this challenge – if your solution does include a chatbot, make sure you have a clear and well-reasoned justification!

4. **Lived Experience Engagement**

Does your solution tackle a high-priority problem in an acceptable way from the patient/user perspective? How will those with lived experience of mental health conditions be involved in the design and development of your solution? The perspectives of those with lived experience will be essential in ensuring that the solution is acceptable, ethical and effective for end users, and should inform multiple stages of the development process.

5. **Safety.** 

Consider how you can ensure that your project makes safe use of generative AI. For example, what monitoring and evaluation methods will you need to ensure that model output is appropriate, and how will you prevent unsafe interactions between models, patients, and clinicians?

### Guidelines
When designing your project and submitting your materials, it is important to bear in mind the following:

1. **Personal data.** You must not use any personal data throughout the process (both when using any tools provided to you or in your submission materials). Personal data is any information that either identifies an individual or could be used to identify an individual and includes, in particular, live patient data. If you need to use data as part of your project, please use dummy or artificial information instead. 
2. **Confidential and proprietary information.** The goal of the hackathon is to share ideas and build an engaged multi-disciplinary community to tackle the challenge of using generative AI to revolutionize mental health measurement. It will be conducted in an open environment with materials available to external mentors and judges as well as other hackathon participants. You should think carefully when producing materials so you do not include confidential or sensitive information that you would not want others to view or access. If you use any information or materials owned by a third party you must ensure you have the rights to use those for the purpose of the hackathon.
3. **Focus on research, not clinical deployment.** The purpose of this hackathon is to propose possible research avenues and solutions to revolutionize mental health measurement. You must not use any tools (including Google’s technology or products) or submit materials in a way that could be determined to be giving clinical advice or creating a diagnostic tool or other medical device. 
4. **Respect others.** We want to maintain a respectful environment for everyone, which means you must follow these basic rules of conduct:
    - comply with applicable laws, including export control and sanctions laws
    - respect the rights of others, including privacy and intellectual property rights 
    - comply with any usage standards or rules relating to any content you submit or use in the hackathon, such as the terms of service for any generative AI technology or tools
    - don’t abuse or harm others, yourself, or any services (or threaten or encourage such abuse or harm) — for example, by misleading, illegally impersonating, defaming, bullying or harassing others or generating or sharing malware 
